{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# exercise 8.2.2\n",
    "import importlib_resources\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "\n",
    "from dtuimldmtools import (\n",
    "    draw_neural_net,\n",
    "    train_neural_net,\n",
    "    visualize_decision_boundary,\n",
    ")\n"
   ],
   "id": "a90c9bf9fe52e377"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# exercise 5.1.1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data\"\n",
    "\n",
    "# Load the SAheart dataset\n",
    "df = pd.read_csv(url, index_col='row.names')\n",
    "\n",
    "\n",
    "# Convert binary text data to numbered categories\n",
    "df['famhist'] = pd.Categorical(df['famhist']).codes\n",
    "y=np.asarray(np.asmatrix(df[\"chd\"].values).T).squeeze()\n",
    "\n",
    "\n",
    "# Attribute names\n",
    "attributeNames = list(map(lambda x: x.capitalize(), df.columns.tolist()))\n",
    "\n",
    "X = df.to_numpy()\n",
    "y = df['typea'].to_numpy().reshape(-1, 1)\n",
    "N, M = X.shape\n",
    "classNames = [0, 1]\n",
    "C = len(classNames)"
   ],
   "id": "f5aedc10f2188872"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# K-fold CrossValidation (4 folds here to speed up this example)\n",
    "K = 4\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "\n",
    "# Setup figure for display of the decision boundary for the several crossvalidation folds.\n",
    "decision_boundaries = plt.figure(1, figsize=(10, 10))\n",
    "# Determine a size of a plot grid that fits visualizations for the chosen number\n",
    "# of cross-validation splits, if K=4, this is simply a 2-by-2 grid.\n",
    "subplot_size_1 = int(np.floor(np.sqrt(K)))\n",
    "subplot_size_2 = int(np.ceil(K / subplot_size_1))\n",
    "# Set overall title for all of the subplots\n",
    "plt.suptitle(\"Data and model decision boundaries\", fontsize=20)\n",
    "# Change spacing of subplots\n",
    "plt.subplots_adjust(left=0, bottom=0, right=1, top=0.9, wspace=0.5, hspace=0.25)\n",
    "\n",
    "# Setup figure for display of learning curves and error rates in fold\n",
    "summaries, summaries_axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# Make a list for storing assigned color of learning curve for up to K=10\n",
    "color_list = [\n",
    "    \"tab:orange\",\n",
    "    \"tab:green\",\n",
    "    \"tab:purple\",\n",
    "    \"tab:brown\",\n",
    "    \"tab:pink\",\n",
    "    \"tab:gray\",\n",
    "    \"tab:olive\",\n",
    "    \"tab:cyan\",\n",
    "    \"tab:red\",\n",
    "    \"tab:blue\",\n",
    "]"
   ],
   "id": "7dba75a531b4cd37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Define the model structure\n",
    "n_hidden_units = 1  # number of hidden units in the signle hidden layer\n",
    "# The lambda-syntax defines an anonymous function, which is used here to\n",
    "# make it easy to make new networks within each cross validation fold\n",
    "model = lambda: torch.nn.Sequential(\n",
    "    torch.nn.Linear(M, n_hidden_units),  # M features to H hiden units\n",
    "    # 1st transfer function, either Tanh or ReLU:\n",
    "    torch.nn.Tanh(),  # torch.nn.ReLU(),\n",
    "    torch.nn.Linear(n_hidden_units, 1),  # H hidden units to 1 output neuron\n",
    "    #add softmax\n",
    "    #torch.nn.Sigmoid(),  # final tranfer function\n",
    ")\n",
    "# Since we're training a neural network for binary classification, we use a\n",
    "# binary cross entropy loss (see the help(train_neural_net) for more on\n",
    "# the loss_fn input to the function)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "# Train for a maximum of 10000 steps, or until convergence (see help for the\n",
    "# function train_neural_net() for more on the tolerance/convergence))\n",
    "max_iter = 10000\n",
    "print(\"Training model of type:\\n{}\\n\".format(str(model())))\n",
    "\n",
    "# Do cross-validation:\n",
    "errors = []  # make a list for storing generalizaition error in each loop\n",
    "# Loop over each cross-validation split. The CV.split-method returns the\n",
    "# indices to be used for training and testing in each split, and calling\n",
    "# the enumerate-method with this simply returns this indices along with\n",
    "# a counter k:"
   ],
   "id": "cc2e62927d3c72ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "for k, (train_index, test_index) in enumerate(CV.split(X, y)):\n",
    "    print(\"\\nCrossvalidation fold: {0}/{1}\".format(k + 1, K))\n",
    "\n",
    "    # Extract training and test set for current CV fold,\n",
    "    # and convert them to PyTorch tensors\n",
    "    X_train = torch.Tensor(X[train_index, :])\n",
    "    y_train = torch.Tensor(y[train_index])\n",
    "    X_test = torch.Tensor(X[test_index, :])\n",
    "    y_test = torch.Tensor(y[test_index])\n",
    "\n",
    "    # Go to the file 'dtuimldmtools.py' in the Tools sub-folder of the toolbox\n",
    "    # and see how the network is trained (search for 'def train_neural_net',\n",
    "    # which is the place the function below is defined)\n",
    "    net, final_loss, learning_curve = train_neural_net(\n",
    "        model, loss_fn, X=X_train, y=y_train, n_replicates=3, max_iter=max_iter\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\tBest loss: {}\\n\".format(final_loss))\n",
    "\n",
    "    # Determine estimated class labels for test set\n",
    "    y_sigmoid = net(X_test)  # activation of final note, i.e. prediction of network\n",
    "    y_test_est =  y_sigmoid # threshold output of sigmoidal function\n",
    "    y_test = y_test\n",
    "    # Determine errors and error rate\n",
    "    e = y_test_est != y_test\n",
    "    error_rate = (sum(e).type(torch.float) / len(y_test)).data.numpy()\n",
    "    errors.append(error_rate)  # store error rate for current CV fold\n",
    "\n",
    "    # Make a subplot for current cross validation fold that displays the\n",
    "    # decision boundary over the original data, \"background color\" corresponds\n",
    "    # to the output of the sigmoidal transfer function (i.e. before threshold),\n",
    "    # white areas are areas of uncertainty, and a deaper red/blue means\n",
    "    # that the network \"is more sure\" of a given class.\n",
    "\n",
    "\n",
    "# Make a subplot for current cross validation fold that displays the\n",
    "    plt.figure(decision_boundaries.number)\n",
    "    plt.subplot(subplot_size_1, subplot_size_2, k + 1)\n",
    "    plt.title(\"CV fold {0}\".format(k + 1), color=color_list[k])\n",
    "    predict = lambda x: net(torch.tensor(x, dtype=torch.float)).data.numpy()\n",
    "    visualize_decision_boundary(\n",
    "        predict,\n",
    "        X,\n",
    "        y,  # provide data, along with function for prediction\n",
    "        attributeNames,\n",
    "        ['tobacco', 'chd'],  # provide information on attribute and class names\n",
    "        train=train_index,\n",
    "        test=test_index,  # provide information on partioning\n",
    "        show_legend=k == (K - 1),\n",
    "    )  # only display legend for last plot\n",
    "\n",
    "    # Display the learning curve for the best net in the current fold\n",
    "    (h,) = summaries_axes[0].plot(learning_curve, color=color_list[k])\n",
    "    h.set_label(\"CV fold {0}\".format(k + 1))\n",
    "    summaries_axes[0].set_xlabel(\"Iterations\")\n",
    "    summaries_axes[0].set_xlim((0, max_iter))\n",
    "    summaries_axes[0].set_ylabel(\"Loss\")\n",
    "    summaries_axes[0].set_title(\"Learning curves\")\n",
    "\n",
    "# Display the error rate across folds\n",
    "\n"
   ],
   "id": "465ddc9bcd0305ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "summaries_axes[1].bar(\n",
    "    np.arange(1, K + 1), np.squeeze(np.asarray(errors)), color=color_list\n",
    ")\n",
    "summaries_axes[1].set_xlabel(\"Fold\")\n",
    "summaries_axes[1].set_xticks(np.arange(1, K + 1))\n",
    "summaries_axes[1].set_ylabel(\"Error rate\")\n",
    "summaries_axes[1].set_title(\"Test misclassification rates\")\n",
    "\n",
    "# Show the plots\n",
    "# plt.show(decision_boundaries.number) # try these lines if the following code fails (depends on package versions)\n",
    "# plt.show(summaries.number)\n",
    "plt.show()\n",
    "\n",
    "# Display a diagram of the best network in last fold\n",
    "print(\"Diagram of best neural net in last fold:\")\n",
    "weights = [net[i].weight.data.numpy().T for i in [0, 2]]\n",
    "biases = [net[i].bias.data.numpy() for i in [0, 2]]\n",
    "tf = [str(net[i]) for i in [1, 3]]\n",
    "draw_neural_net(weights, biases, tf)\n",
    "\n",
    "# Print the average classification error rate\n",
    "print(\n",
    "    \"\\nGeneralization error/average error rate: {0}%\".format(\n",
    "        round(100 * np.mean(errors), 4)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Ran exercise 8.2.2.\")"
   ],
   "id": "f35d8b1f9be7ade5"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
