{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# exercise 5.1.1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data\"\n",
    "\n",
    "# Load the SAheart dataset\n",
    "df = pd.read_csv(url, index_col='row.names')\n",
    "\n",
    "\n",
    "# Convert binary text data to numbered categories\n",
    "df['famhist'] = pd.Categorical(df['famhist']).codes\n",
    "y=np.asarray(np.asmatrix(df[\"chd\"].values).T).squeeze()\n",
    "df = df.drop(columns=[\"chd\"])\n",
    "\n",
    "# Attribute names\n",
    "attributeNames = list(map(lambda x: x.capitalize(), df.columns.tolist()))\n",
    "\n",
    "X = df.to_numpy()\n",
    "N, M = X.shape\n",
    "\n",
    "\n"
   ],
   "id": "4abc04629a14445e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    parameters = {\n",
    "        \"W1\": np.random.randn(hidden_size, input_size) * 0.01,\n",
    "        \"b1\": np.zeros((hidden_size, 1)),\n",
    "        \"W2\": np.random.randn(output_size, hidden_size) * 0.01,\n",
    "        \"b2\": np.zeros((output_size, 1))\n",
    "    }\n",
    "    return parameters\n"
   ],
   "id": "82985951b1554222"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return (Z > 0).astype(int)\n"
   ],
   "id": "3d5b833268989bac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    # First layer\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)  # or another activation function\n",
    "\n",
    "    # Output layer\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = 1 / (1 + np.exp(-Z2))  # sigmoid activation\n",
    "\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "\n",
    "    return A2, cache\n"
   ],
   "id": "5b44ba830d5b97d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_cost(Y, A2):\n",
    "    m = Y.shape[1]\n",
    "    cost = np.sum((A2 - Y) ** 2) / (2 * m)\n",
    "    return cost"
   ],
   "id": "91310b49f22c0e27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def backward_propagation(X, Y, parameters, cache):\n",
    "    m = X.shape[1]\n",
    "    W2 = parameters[\"W2\"]\n",
    "\n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "    dW2 = np.dot(dZ2, cache[\"A1\"].T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "\n",
    "    dZ1 = np.dot(W2.T, dZ2) * relu_derivative(cache[\"Z1\"])\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads\n"
   ],
   "id": "ceb9844d0136f9f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    for key in parameters.keys():\n",
    "        parameters[key] -= learning_rate * grads[\"d\" + key]\n",
    "    return parameters\n"
   ],
   "id": "a06fc3c03d68446d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_neural_network(X, Y, input_size, hidden_size, output_size, epochs=1000, learning_rate=0.01):\n",
    "    parameters = initialize_parameters(input_size, hidden_size, output_size)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(Y, A2)\n",
    "        grads = backward_propagation(X, Y, parameters, cache)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {i}: Cost = {cost}\")\n",
    "\n",
    "    return parameters\n"
   ],
   "id": "3cd2a25ad3ece66b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict(X, parameters):\n",
    "    A2, _ = forward_propagation(X, parameters)\n",
    "    return (A2 > 0.5).astype(int)\n"
   ],
   "id": "fc83c28e43e61f8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(X)",
   "id": "47cc72877eb91e5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Y= np.array(df['typea'])\n",
   "id": "ae7a3302f2d96f5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Neural Network Implementation\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    Initialize the weights and biases for a 2-layer neural network\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Use He initialization for better convergence\n",
    "    W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2 / input_size)\n",
    "    b1 = np.zeros((hidden_size, 1))\n",
    "    W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2 / hidden_size)\n",
    "    b2 = np.zeros((output_size, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \"\"\"\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Forward propagation through the network\n",
    "    \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    # First layer with ReLU activation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    # Output layer with sigmoid activation for binary classification\n",
    "    # or linear activation for regression\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = Z2  # Linear activation for regression\n",
    "\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "\n",
    "    return A2, cache\n",
    "\n",
    "def compute_cost(Y, A2):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = np.sum((A2 - Y) ** 2) / (2 * m)\n",
    "    return cost\n",
    "\n",
    "def backward_propagation(X, Y, parameters, cache):\n",
    "    \"\"\"\n",
    "    Backward propagation to compute gradients\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    Z1 = cache[\"Z1\"]\n",
    "\n",
    "    # Output layer gradient\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "\n",
    "    # Hidden layer gradient with ReLU derivative\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = dA1 * (Z1 > 0)  # ReLU derivative\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "    gradients = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    dW1 = gradients[\"dW1\"]\n",
    "    db1 = gradients[\"db1\"]\n",
    "    dW2 = gradients[\"dW2\"]\n",
    "    db2 = gradients[\"db2\"]\n",
    "\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def train_neural_network(X, Y, input_size, hidden_size, output_size, epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train the neural network\n",
    "    \"\"\"\n",
    "    parameters = initialize_parameters(input_size, hidden_size, output_size)\n",
    "\n",
    "    costs = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(Y, A2)\n",
    "        gradients = backward_propagation(X, Y, parameters, cache)\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "    return parameters, costs\n",
    "\n",
    "def predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Make predictions with the trained network\n",
    "    \"\"\"\n",
    "    A2, _ = forward_propagation(X, parameters)\n",
    "    return A2\n",
    "\n",
    "# Helper functions\n",
    "def preprocess_data(X_df, Y_array):\n",
    "    \"\"\"\n",
    "    Preprocess the data for the neural network\n",
    "    \"\"\"\n",
    "    # Convert dataframe to numpy array if it's not already\n",
    "    X = X_df.values if isinstance(X_df, pd.DataFrame) else X_df\n",
    "\n",
    "    # Normalize the features (important for neural networks)\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "    # Reshape Y to be a row vector (1, m)\n",
    "    Y = Y_array.reshape(1, -1) if Y_array.ndim == 1 else Y_array\n",
    "\n",
    "    # Transpose X to shape (n_features, n_samples)\n",
    "    X = X.T\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def run_neural_network(X_df, Y_array, hidden_size=9, epochs=10000, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Run the complete neural network training and prediction process\n",
    "    \"\"\"\n",
    "    # Preprocess data\n",
    "    X, Y = preprocess_data(X_df, Y_array)\n",
    "\n",
    "    # Get dimensions\n",
    "    input_size = X.shape[0]\n",
    "    output_size = 1\n",
    "\n",
    "    print(f\"Input shape: {X.shape}\")\n",
    "    print(f\"Output shape: {Y.shape}\")\n",
    "\n",
    "    # Train the network\n",
    "    parameters, costs = train_neural_network(\n",
    "        X, Y, input_size, hidden_size, output_size, epochs, learning_rate\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = predict(X, parameters)\n",
    "\n",
    "    # Calculate R-squared for regression\n",
    "    SSres = np.sum((Y - predictions) ** 2)\n",
    "    SStot = np.sum((Y - np.mean(Y)) ** 2)\n",
    "    r_squared = 1 - (SSres / SStot)\n",
    "\n",
    "    print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "    # Plot the cost over iterations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(0, epochs, 100), costs)\n",
    "    plt.xlabel(\"Iterations (hundreds)\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Cost over Iterations\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return parameters, predictions, r_squared\n",
    "\n",
    "# Example usage\n",
    "# run_neural_network(X_df, Y_array, hidden_size=9, epochs=10000, learning_rate=0.01)"
   ],
   "id": "edc8eea3cb8b9297"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "run_neural_network(X, Y, hidden_size=9, epochs=10000, learning_rate=0.01)",
   "id": "efadb734ea5cdefe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_reshaped = Y.reshape(1, -1)  # Shape becomes (1, 462)\n",
    "X_transposed = X.T  # Shape becomes (9, 462)\n",
    "Y_reshaped = Y.reshape(1, -1)  # Shape becomes (1, 462)\n",
    "trained_parameters = train_neural_network(X_transposed, Y_reshaped, input_size=9, hidden_size=9, output_size=1, epochs=10000, learning_rate=0.1)"
   ],
   "id": "e5cfa9b999287674"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b875851747e82a3"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
